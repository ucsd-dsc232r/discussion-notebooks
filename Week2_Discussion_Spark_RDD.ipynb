{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdW-zFk_qihN"
      },
      "source": [
        "# DSC 232R: Foundations & Applications of PySpark RDDs\n",
        "\n",
        "This notebook covers the basics of Spark and more complex data processing patterns.\n",
        "\n",
        "### Learning Objectives:\n",
        "1. **Environment**: Setting up Spark on Google Colab.\n",
        "2. **Basics**: Creating RDDs and understanding Lazy Evaluation.\n",
        "3. **Transformations**: Deep dive into `map`, `filter`, and `flatMap`.\n",
        "4. **Actions**: Triggering the computation graph.\n",
        "5. **Pair RDDs**: Handling Key-Value data (Aggregations & Joins).\n",
        "6. **Advanced**: Set operations, Caching, and Real-world Log Analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULu0d0sBqihO"
      },
      "source": [
        "## 1. Installation and SparkContext\n",
        "Spark runs on the JVM, so we must install Java 8 and the Spark binaries first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awc7W7uGqihO",
        "outputId": "69a4037f-b00c-48ec-e754-e4d0c26972d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Context Initialized!\n"
          ]
        }
      ],
      "source": [
        "# Install Java and Spark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "conf = SparkConf().setAppName(\"DSC232R_Discussion\").setMaster(\"local[*]\")\n",
        "sc = SparkContext(conf=conf)\n",
        "print(\"Spark Context Initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbf44lTMqihP"
      },
      "source": [
        "## 2. Creating RDDs (The Basics)\n",
        "There are two primary ways to get data into Spark."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is an RDD?\n",
        "* **Resilient**: Fault-tolerant, able to recompute missing partitions.\n",
        "* **Distributed**: Data is partitioned across multiple nodes in a cluster.\n",
        "* **Dataset**: A collection of objects."
      ],
      "metadata": {
        "id": "BASJaYOuN_po"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Spark Architecture\n",
        "\n",
        "When you execute code, this is how Spark distributes the work:\n",
        "\n",
        "```text\n",
        "       [ Driver Program ]\n",
        "              | (SparkContext)\n",
        "     --------------------------\n",
        "     |            |           |\n",
        " [ Worker ]   [ Worker ]  [ Worker ]\n",
        " (Executor)   (Executor)  (Executor)\n",
        "    /   \\       /   \\       /   \\\n",
        " [Task][Task] [Task][Task] [Task][Task]\n",
        "```\n",
        "\n",
        "**Transformations** build the plan (DAG). **Actions** trigger the execution."
      ],
      "metadata": {
        "id": "NnOQb_sMOBmo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2bwL7RHqihQ",
        "outputId": "9d7e3b6f-71fa-4613-85b0-ccc943a17c13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD from List: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
          ]
        }
      ],
      "source": [
        "# Method 1: Parallelizing an existing Python collection\n",
        "data_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "rdd_from_list = sc.parallelize(data_list)\n",
        "print(\"RDD from List:\", rdd_from_list.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5ZCEh_YqihQ",
        "outputId": "1e7049f3-ab81-4471-89c3-353d44dabb3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw RDD (Strings): ['10', '20', '30', '40', '50']\n",
            "Integer RDD: [10, 20, 30, 40, 50]\n"
          ]
        }
      ],
      "source": [
        "# Method 2: Reading from an external file\n",
        "# Step A: We generate 'numbers.txt' locally first so this code is self-contained\n",
        "with open(\"numbers.txt\", \"w\") as f:\n",
        "    f.write(\"10\\n20\\n30\\n40\\n50\")\n",
        "\n",
        "# Step B: Load the file into an RDD\n",
        "file_rdd = sc.textFile(\"numbers.txt\")\n",
        "print(\"Raw RDD (Strings):\", file_rdd.collect())\n",
        "\n",
        "# Step C: Convert strings to integers for calculation\n",
        "int_rdd = file_rdd.map(lambda x: int(x))\n",
        "print(\"Integer RDD:\", int_rdd.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBRyutjtqihQ"
      },
      "source": [
        "## 3. Transformations: The Step-by-Step Pipeline\n",
        "\n",
        "### Concept: Lazy Evaluation\n",
        "Transformations do **not** execute immediately. They only build a recipe (the DAG).\n",
        "\n",
        "Let's use the **Bookstore Example**: we want to find books that cost more than $40 after a 10% tax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXhyKKgHqihQ",
        "outputId": "14900aaa-4c1a-4a49-989f-6b141051b977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Books with Tax: [('Spark Basics', 38.5), ('Python for Data Science', 50.05), ('Distributed Systems', 66.0), ('Big Data 101', 27.5)]\n"
          ]
        }
      ],
      "source": [
        "books_data = [\n",
        "    (\"Spark Basics\", 35.00),\n",
        "    (\"Python for Data Science\", 45.50),\n",
        "    (\"Distributed Systems\", 60.00),\n",
        "    (\"Big Data 101\", 25.00)\n",
        "]\n",
        "books_rdd = sc.parallelize(books_data)\n",
        "\n",
        "# Step 1: MAP - Apply 10% Tax\n",
        "taxed_rdd = books_rdd.map(lambda x: (x[0], round(x[1] * 1.1, 2)))\n",
        "\n",
        "# Show intermediate output to students\n",
        "print(\"1. Books with Tax:\", taxed_rdd.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8VssyzCqihQ",
        "outputId": "840ee371-2f7e-4376-f76a-b25f289e8d37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2. Expensive Books Only: [('Python for Data Science', 50.05), ('Distributed Systems', 66.0)]\n"
          ]
        }
      ],
      "source": [
        "# Step 2: FILTER - Filter out books <= $40\n",
        "expensive_books_rdd = taxed_rdd.filter(lambda x: x[1] > 40.00)\n",
        "\n",
        "# Show final transformed output\n",
        "print(\"2. Expensive Books Only:\", expensive_books_rdd.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws5h53AEqihQ"
      },
      "source": [
        "### FlatMap vs. Map\n",
        "`map` keeps the 1-to-1 relationship. `flatMap` \"flattens\" the structure (1-to-many)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LSuxSxUqihQ",
        "outputId": "f645752a-141c-4db2-8b49-10de8a1c00f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Map (nested list): [['Hello', 'Spark'], ['Goodbye', 'Hadoop']]\n",
            "FlatMap (flat list): ['Hello', 'Spark', 'Goodbye', 'Hadoop']\n"
          ]
        }
      ],
      "source": [
        "sentences = [\"Hello Spark\", \"Goodbye Hadoop\"]\n",
        "sentences_rdd = sc.parallelize(sentences)\n",
        "\n",
        "map_res = sentences_rdd.map(lambda x: x.split(\" \")).collect()\n",
        "flat_res = sentences_rdd.flatMap(lambda x: x.split(\" \")).collect()\n",
        "\n",
        "print(\"Map (nested list):\", map_res)\n",
        "print(\"FlatMap (flat list):\", flat_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z02BzjIVqihR"
      },
      "source": [
        "## 4. RDD Actions\n",
        "Actions trigger the actual computation. Until you call these, Spark has done zero work on the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_oMqiS2qihR",
        "outputId": "d58a8581-dcb6-4dda-f4ff-6b6aafdf015d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count: 5\n",
            "First item: 10\n",
            "Take 2 items: [10, 20]\n",
            "Sum (using Reduce): 150\n"
          ]
        }
      ],
      "source": [
        "test_rdd = sc.parallelize([10, 20, 30, 40, 50])\n",
        "\n",
        "print(\"Count:\", test_rdd.count())\n",
        "print(\"First item:\", test_rdd.first())\n",
        "print(\"Take 2 items:\", test_rdd.take(2))\n",
        "print(\"Sum (using Reduce):\", test_rdd.reduce(lambda a, b: a + b))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx_F59V3qihR"
      },
      "source": [
        "## 5. Pair RDDs: Working with Keys\n",
        "This is where Spark becomes powerful for Big Data aggregations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXfEa1ZfqihR",
        "outputId": "982bfa5b-d6bc-425b-dfc9-88a0d932ba42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sales per Author: [('Jane Smith', 125), ('John Doe', 120), ('Bob Johnson', 50)]\n"
          ]
        }
      ],
      "source": [
        "# Sales Data: (Author, Transaction Amount)\n",
        "author_sales = [\n",
        "    (\"John Doe\", 75), (\"Jane Smith\", 100),\n",
        "    (\"John Doe\", 45), (\"Bob Johnson\", 50),\n",
        "    (\"Jane Smith\", 25)\n",
        "]\n",
        "sales_rdd = sc.parallelize(author_sales)\n",
        "\n",
        "# reduceByKey: Sum the sales for each author\n",
        "author_totals = sales_rdd.reduceByKey(lambda a, b: a + b)\n",
        "print(\"Total Sales per Author:\", author_totals.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX29V2ecqihR",
        "outputId": "a771e3b1-7e92-47cd-cb4e-646fb12657b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joined (Author, (Total Sales, Rating)): [('Jane Smith', (125, 4.8)), ('John Doe', (120, 4.5)), ('Bob Johnson', (50, 4.2))]\n"
          ]
        }
      ],
      "source": [
        "# Join: Adding Author Ratings to the Sales Totals\n",
        "author_ratings = sc.parallelize([\n",
        "    (\"John Doe\", 4.5), (\"Jane Smith\", 4.8), (\"Bob Johnson\", 4.2)\n",
        "])\n",
        "\n",
        "joined_rdd = author_totals.join(author_ratings)\n",
        "print(\"Joined (Author, (Total Sales, Rating)):\", joined_rdd.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfW8ZO8QqihR"
      },
      "source": [
        "## 6. Advanced Concepts\n",
        "\n",
        "### Set Operations and Caching\n",
        "Set operations like `union` and `intersection` are useful for user-group comparisons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ5VBIAvqihR",
        "outputId": "b99fdd57-b836-4e57-be37-b1126181aa43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Union (All unique users): ['User2', 'User3', 'User5', 'User1', 'User4']\n",
            "Intersection (Common users): ['User3']\n",
            "Group A is now cached.\n"
          ]
        }
      ],
      "source": [
        "group_a = sc.parallelize([\"User1\", \"User2\", \"User3\"])\n",
        "group_b = sc.parallelize([\"User3\", \"User4\", \"User5\"])\n",
        "\n",
        "print(\"Union (All unique users):\", group_a.union(group_b).distinct().collect())\n",
        "print(\"Intersection (Common users):\", group_a.intersection(group_b).collect())\n",
        "\n",
        "# CACHING\n",
        "# If we use an RDD multiple times, we cache it in memory for performance.\n",
        "group_a.cache()\n",
        "print(\"Group A is now cached.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6GyKjKhqihR"
      },
      "source": [
        "### Practical Exercise: Production-Style Log Analysis\n",
        "Given raw logs, we want to find the most frequent ERROR message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeXq9fFqqihR",
        "outputId": "f4c40f36-8fd9-4571-a174-a849b9de5893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log Analysis Result: [('Timeout', 3), ('DB Fail', 1)]\n"
          ]
        }
      ],
      "source": [
        "raw_logs = [\n",
        "    \"[ERROR] - Timeout\", \"[INFO] - Boot\", \"[ERROR] - Timeout\",\n",
        "    \"[ERROR] - DB Fail\", \"[WARNING] - High Mem\", \"[ERROR] - Timeout\"\n",
        "]\n",
        "logs_rdd = sc.parallelize(raw_logs)\n",
        "\n",
        "# The Pipeline:\n",
        "# 1. Filter only Errors\n",
        "# 2. Extract the message string\n",
        "# 3. Pair with a count of 1\n",
        "# 4. Sum counts by message\n",
        "# 5. Sort by frequency\n",
        "\n",
        "result = logs_rdd \\\n",
        "    .filter(lambda line: \"[ERROR]\" in line) \\\n",
        "    .map(lambda line: (line.split(\" - \")[1], 1)) \\\n",
        "    .reduceByKey(lambda a, b: a + b) \\\n",
        "    .sortBy(lambda x: x[1], ascending=False)\n",
        "\n",
        "print(\"Log Analysis Result:\", result.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YVzagzTqihS"
      },
      "source": [
        "## Diagram: Understanding the DAG and Lineage\n",
        "\n",
        "```text\n",
        "[ Parallelize ] -> RDD_A\n",
        "     |\n",
        "     v\n",
        "[ map(tax) ]   -> RDD_B (Depends on RDD_A)\n",
        "     |\n",
        "     v\n",
        "[ filter(40) ] -> RDD_C (Depends on RDD_B)\n",
        "     |\n",
        "     v\n",
        "[ collect() ]  <- ACTION (Triggers re-calculation of A -> B -> C)\n",
        "```\n",
        "\n",
        "**Fault Tolerance**: If RDD_B is lost, Spark uses the graph above to recompute it from RDD_A!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}