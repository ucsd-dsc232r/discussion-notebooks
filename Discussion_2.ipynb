{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "jx80PfnxeYv7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N78xCpBVaOgL",
        "outputId": "e67026c5-710c-459f-9cbf-749d99fc771c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\u001b[33m\r0% [Waiting for headers] [1 InRelease 14.2 kB/129 kB 11%] [Connected to cloud.r\u001b[0m\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [1 InRelease 43.1 kB/129 kB 33%] [Connected to cloud.r-project.org (108.157.\u001b[0m\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\u001b[33m\r0% [3 InRelease 15.6 kB/128 kB 12%] [1 InRelease 43.1 kB/129 kB 33%] [Connected\u001b[0m\u001b[33m\r0% [3 InRelease 54.7 kB/128 kB 43%] [Waiting for headers] [Waiting for headers]\u001b[0m\r                                                                               \rHit:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\u001b[33m\r0% [3 InRelease 60.5 kB/128 kB 47%] [Waiting for headers] [Waiting for headers]\u001b[0m\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connectin\u001b[0m\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 257 kB in 2s (162 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "36 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "\n",
        "import os\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "oW_xuMwL2thp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DataFrame Tutorial\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "MZXILRJM4uhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DataFrames\n",
        "Apache Spark DataFrames are distributed collections of data organized into named columns. They are conceptually equivalent to tables in a relational database or DataFrames in Python's pandas library, but with the added benefit of being distributed across a cluster for big data processing."
      ],
      "metadata": {
        "id": "B2oVRSqg9AdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating DataFrames\n"
      ],
      "metadata": {
        "id": "hTqLyktk58_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. From a list of data"
      ],
      "metadata": {
        "id": "o4reXhuN6A1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame from a list\n",
        "data = [(\"John\", 25, \"New York\"),\n",
        "        (\"Jane\", 30, \"San Francisco\"),\n",
        "        (\"Mike\", 35, \"Chicago\")]\n",
        "\n",
        "columns = [\"name\", \"age\", \"city\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7NUswfZ4xD3",
        "outputId": "9b701143-b75c-45b4-d68e-a96efc7d6e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-------------+\n",
            "|name|age|         city|\n",
            "+----+---+-------------+\n",
            "|John| 25|     New York|\n",
            "|Jane| 30|San Francisco|\n",
            "|Mike| 35|      Chicago|\n",
            "+----+---+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Using a schema"
      ],
      "metadata": {
        "id": "LgxP3cr36F1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "# Define schema\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), nullable=False),\n",
        "    StructField(\"age\", IntegerType(), nullable=False),\n",
        "    StructField(\"city\", StringType(), nullable=True)\n",
        "])\n",
        "\n",
        "# Create DataFrame with schema\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9eSuYwe4z7s",
        "outputId": "06d41e15-e56a-4cd7-e4f9-eb256560399c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-------------+\n",
            "|name|age|         city|\n",
            "+----+---+-------------+\n",
            "|John| 25|     New York|\n",
            "|Jane| 30|San Francisco|\n",
            "|Mike| 35|      Chicago|\n",
            "+----+---+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. From external data sources\n",
        "\n",
        "\n",
        "```\n",
        "# CSV\n",
        "df_csv = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# JSON\n",
        "df_json = spark.read.json(\"path/to/file.json\")\n",
        "\n",
        "# Parquet\n",
        "df_parquet = spark.read.parquet(\"path/to/file.parquet\")\n",
        "\n",
        "# Database\n",
        "df_jdbc = spark.read \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", \"jdbc:postgresql://localhost:5432/database\") \\\n",
        "    .option(\"dbtable\", \"table_name\") \\\n",
        "    .option(\"user\", \"username\") \\\n",
        "    .option(\"password\", \"password\") \\\n",
        "    .load()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "DWrgiNhs6JZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Basic DataFrame Operations\n",
        "1. Viewing Data"
      ],
      "metadata": {
        "id": "QsYU7bpM6RD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows\n",
        "df.show()\n",
        "\n",
        "# Display the first n rows\n",
        "df.show(5)\n",
        "\n",
        "# Show the schema - similar to pandas df.dtypes\n",
        "df.printSchema()\n",
        "\n",
        "# Get basic statistics - similar to pandas df.describe()\n",
        "df.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjSCn93549L7",
        "outputId": "248f72ee-f141-4531-aab7-8585563ae755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-------------+\n",
            "|name|age|         city|\n",
            "+----+---+-------------+\n",
            "|John| 25|     New York|\n",
            "|Jane| 30|San Francisco|\n",
            "|Mike| 35|      Chicago|\n",
            "+----+---+-------------+\n",
            "\n",
            "+----+---+-------------+\n",
            "|name|age|         city|\n",
            "+----+---+-------------+\n",
            "|John| 25|     New York|\n",
            "|Jane| 30|San Francisco|\n",
            "|Mike| 35|      Chicago|\n",
            "+----+---+-------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = false)\n",
            " |-- age: integer (nullable = false)\n",
            " |-- city: string (nullable = true)\n",
            "\n",
            "+-------+----+----+-------------+\n",
            "|summary|name| age|         city|\n",
            "+-------+----+----+-------------+\n",
            "|  count|   3|   3|            3|\n",
            "|   mean|NULL|30.0|         NULL|\n",
            "| stddev|NULL| 5.0|         NULL|\n",
            "|    min|Jane|  25|      Chicago|\n",
            "|    max|Mike|  35|San Francisco|\n",
            "+-------+----+----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Selecting Columns"
      ],
      "metadata": {
        "id": "0cGrI0KE6UkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Select a single column\n",
        "df.select(\"name\").show()\n",
        "\n",
        "# Select multiple columns\n",
        "df.select(\"name\", \"age\").show()\n",
        "\n",
        "# Select with expressions\n",
        "df.select(F.col(\"name\"), F.col(\"age\") + 1).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13NBdBzq5F7E",
        "outputId": "ee5b9211-8f14-458d-ef21-3a8b98d0668e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "|John|\n",
            "|Jane|\n",
            "|Mike|\n",
            "+----+\n",
            "\n",
            "+----+---+\n",
            "|name|age|\n",
            "+----+---+\n",
            "|John| 25|\n",
            "|Jane| 30|\n",
            "|Mike| 35|\n",
            "+----+---+\n",
            "\n",
            "+----+---------+\n",
            "|name|(age + 1)|\n",
            "+----+---------+\n",
            "|John|       26|\n",
            "|Jane|       31|\n",
            "|Mike|       36|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Filtering Data"
      ],
      "metadata": {
        "id": "tg2_w1zr6XQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter by condition\n",
        "df.filter(df.age > 25).show()\n",
        "\n",
        "# Multiple conditions\n",
        "df.filter((df.age > 25) & (df.city == \"Chicago\")).show()\n",
        "\n",
        "# Using SQL expression\n",
        "df.filter(\"age > 25 AND city = 'Chicago'\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IHFCnk95H7d",
        "outputId": "ac685b1d-d34c-424a-800d-3c4eda70f3a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-------------+\n",
            "|name|age|         city|\n",
            "+----+---+-------------+\n",
            "|Jane| 30|San Francisco|\n",
            "|Mike| 35|      Chicago|\n",
            "+----+---+-------------+\n",
            "\n",
            "+----+---+-------+\n",
            "|name|age|   city|\n",
            "+----+---+-------+\n",
            "|Mike| 35|Chicago|\n",
            "+----+---+-------+\n",
            "\n",
            "+----+---+-------+\n",
            "|name|age|   city|\n",
            "+----+---+-------+\n",
            "|Mike| 35|Chicago|\n",
            "+----+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Adding and Modifying Columns"
      ],
      "metadata": {
        "id": "lzzJq2xG6Z-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new column\n",
        "df = df.withColumn(\"age_plus_ten\", df.age + 10)\n",
        "df.show()\n",
        "\n",
        "# Rename a column\n",
        "df = df.withColumnRenamed(\"age\", \"years_old\")\n",
        "df.show()\n",
        "\n",
        "# Drop a column\n",
        "df = df.drop(\"age_plus_ten\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VakOpQuw5Jqr",
        "outputId": "20f4815c-e4d1-46fb-95c6-7b87b0a89dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-------------+------------+\n",
            "|name|age|         city|age_plus_ten|\n",
            "+----+---+-------------+------------+\n",
            "|John| 25|     New York|          35|\n",
            "|Jane| 30|San Francisco|          40|\n",
            "|Mike| 35|      Chicago|          45|\n",
            "+----+---+-------------+------------+\n",
            "\n",
            "+----+---------+-------------+------------+\n",
            "|name|years_old|         city|age_plus_ten|\n",
            "+----+---------+-------------+------------+\n",
            "|John|       25|     New York|          35|\n",
            "|Jane|       30|San Francisco|          40|\n",
            "|Mike|       35|      Chicago|          45|\n",
            "+----+---------+-------------+------------+\n",
            "\n",
            "+----+---------+-------------+\n",
            "|name|years_old|         city|\n",
            "+----+---------+-------------+\n",
            "|John|       25|     New York|\n",
            "|Jane|       30|San Francisco|\n",
            "|Mike|       35|      Chicago|\n",
            "+----+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Advanced DataFrame Operations"
      ],
      "metadata": {
        "id": "TA7O_X49_Z3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Grouping and Aggregation"
      ],
      "metadata": {
        "id": "XHQWSMPY6csu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by and count\n",
        "df.groupBy(\"city\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9yppugH5MZF",
        "outputId": "198a960e-9b31-47ce-d1ab-53aa4c3148f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+\n",
            "|         city|count|\n",
            "+-------------+-----+\n",
            "|     New York|    1|\n",
            "|San Francisco|    1|\n",
            "|      Chicago|    1|\n",
            "+-------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"city\").agg(\n",
        "    F.count(\"*\").alias(\"count\"),\n",
        "    F.avg(\"years_old\").alias(\"avg_age\"),\n",
        "    F.min(\"years_old\").alias(\"min_age\"),\n",
        "    F.max(\"years_old\").alias(\"max_age\")\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfTgKk8wYBgg",
        "outputId": "c6481ae2-5ad6-409b-c569-950aaa8530b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+-------+-------+-------+\n",
            "|         city|count|avg_age|min_age|max_age|\n",
            "+-------------+-----+-------+-------+-------+\n",
            "|     New York|    1|   25.0|     25|     25|\n",
            "|San Francisco|    1|   30.0|     30|     30|\n",
            "|      Chicago|    1|   35.0|     35|     35|\n",
            "+-------------+-----+-------+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Joining DataFrames"
      ],
      "metadata": {
        "id": "PjpBv5vJ6f0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create another DataFrame\n",
        "employee_data = [\n",
        "    (1, \"John\", \"Engineering\"),\n",
        "    (2, \"Jane\", \"Marketing\"),\n",
        "    (3, \"Mike\", \"Sales\")\n",
        "]\n",
        "employee_df = spark.createDataFrame(employee_data, [\"id\", \"name\", \"department\"])\n",
        "\n",
        "salary_data = [\n",
        "    (1, 70000),\n",
        "    (2, 80000),\n",
        "    (3, 65000)\n",
        "]\n",
        "salary_df = spark.createDataFrame(salary_data, [\"id\", \"salary\"])\n",
        "\n",
        "# Inner Join\n",
        "employee_df.join(salary_df, \"id\").show()\n",
        "\n",
        "# Left Join\n",
        "employee_df.join(salary_df, \"id\", \"left\").show()\n",
        "\n",
        "# Right Join\n",
        "employee_df.join(salary_df, \"id\", \"right\").show()\n",
        "\n",
        "# Full Outer Join\n",
        "employee_df.join(salary_df, \"id\", \"outer\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJrcdH805OQ_",
        "outputId": "9bccfdeb-c604-4b47-a6a5-0e3a08f6e449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----------+------+\n",
            "| id|name| department|salary|\n",
            "+---+----+-----------+------+\n",
            "|  1|John|Engineering| 70000|\n",
            "|  2|Jane|  Marketing| 80000|\n",
            "|  3|Mike|      Sales| 65000|\n",
            "+---+----+-----------+------+\n",
            "\n",
            "+---+----+-----------+------+\n",
            "| id|name| department|salary|\n",
            "+---+----+-----------+------+\n",
            "|  1|John|Engineering| 70000|\n",
            "|  3|Mike|      Sales| 65000|\n",
            "|  2|Jane|  Marketing| 80000|\n",
            "+---+----+-----------+------+\n",
            "\n",
            "+---+----+-----------+------+\n",
            "| id|name| department|salary|\n",
            "+---+----+-----------+------+\n",
            "|  1|John|Engineering| 70000|\n",
            "|  3|Mike|      Sales| 65000|\n",
            "|  2|Jane|  Marketing| 80000|\n",
            "+---+----+-----------+------+\n",
            "\n",
            "+---+----+-----------+------+\n",
            "| id|name| department|salary|\n",
            "+---+----+-----------+------+\n",
            "|  1|John|Engineering| 70000|\n",
            "|  2|Jane|  Marketing| 80000|\n",
            "|  3|Mike|      Sales| 65000|\n",
            "+---+----+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Handling Missing Values"
      ],
      "metadata": {
        "id": "ndcc_Sge6lbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame with null values\n",
        "data_with_nulls = [\n",
        "    (\"John\", 25, \"New York\"),\n",
        "    (\"Jane\", None, \"San Francisco\"),\n",
        "    (\"Mike\", 35, None),\n",
        "    (None, 40, \"Boston\")\n",
        "]\n",
        "df_nulls = spark.createDataFrame(data_with_nulls, [\"name\", \"age\", \"city\"])\n",
        "\n",
        "# Drop rows with any null values\n",
        "df_nulls.na.drop().show()\n",
        "\n",
        "# Drop rows with null values in specific columns\n",
        "df_nulls.na.drop(subset=[\"name\"]).show()\n",
        "\n",
        "# Fill null values\n",
        "df_nulls.na.fill({\"age\": 0, \"name\": \"Unknown\", \"city\": \"Unknown\"}).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8capSclM5Qie",
        "outputId": "e0fe1ee5-cffa-4928-f9f9-36c034086cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+--------+\n",
            "|name|age|    city|\n",
            "+----+---+--------+\n",
            "|John| 25|New York|\n",
            "+----+---+--------+\n",
            "\n",
            "+----+----+-------------+\n",
            "|name| age|         city|\n",
            "+----+----+-------------+\n",
            "|John|  25|     New York|\n",
            "|Jane|NULL|San Francisco|\n",
            "|Mike|  35|         NULL|\n",
            "+----+----+-------------+\n",
            "\n",
            "+-------+---+-------------+\n",
            "|   name|age|         city|\n",
            "+-------+---+-------------+\n",
            "|   John| 25|     New York|\n",
            "|   Jane|  0|San Francisco|\n",
            "|   Mike| 35|      Unknown|\n",
            "|Unknown| 40|       Boston|\n",
            "+-------+---+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. User Defined Functions"
      ],
      "metadata": {
        "id": "ZBN406EmGqi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "df.show()\n",
        "\n",
        "# Define a UDF\n",
        "def age_category(age):\n",
        "    if age is None:\n",
        "        return \"Unknown\"\n",
        "    elif age < 30:\n",
        "        return \"Young\"\n",
        "    else:\n",
        "        return \"Senior\"\n",
        "\n",
        "# Register UDF\n",
        "age_category_udf = udf(age_category, StringType())\n",
        "\n",
        "# Apply UDF\n",
        "df.withColumn(\"age_category\", age_category_udf(df.years_old)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc4dfhZXGqFU",
        "outputId": "1bedd549-e271-4e37-89fe-86441d7b512c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+-------------+\n",
            "|name|years_old|         city|\n",
            "+----+---------+-------------+\n",
            "|John|       25|     New York|\n",
            "|Jane|       30|San Francisco|\n",
            "|Mike|       35|      Chicago|\n",
            "+----+---------+-------------+\n",
            "\n",
            "+----+---------+-------------+------------+\n",
            "|name|years_old|         city|age_category|\n",
            "+----+---------+-------------+------------+\n",
            "|John|       25|     New York|       Young|\n",
            "|Jane|       30|San Francisco|      Senior|\n",
            "|Mike|       35|      Chicago|      Senior|\n",
            "+----+---------+-------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write DataFrames to files\n",
        "\n",
        "\n",
        "```\n",
        "# Save as CSV\n",
        "df.write.csv(\"path/to/output/csv\", header=True)\n",
        "\n",
        "# Save as JSON\n",
        "df.write.json(\"path/to/output/json\")\n",
        "\n",
        "# Save as Parquet\n",
        "df.write.parquet(\"path/to/output/parquet\")\n",
        "\n",
        "# Save to a database\n",
        "df.write \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", \"jdbc:postgresql://localhost:5432/database\") \\\n",
        "    .option(\"dbtable\", \"output_table\") \\\n",
        "    .option(\"user\", \"username\") \\\n",
        "    .option(\"password\", \"password\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "SpOqvFWpGafG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Resources For DataFrames\n",
        "https://sparkbyexamples.com/pyspark-tutorial/"
      ],
      "metadata": {
        "id": "BFoFod6iDvS4"
      }
    }
  ]
}