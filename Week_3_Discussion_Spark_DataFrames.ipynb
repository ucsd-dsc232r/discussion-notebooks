{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **DSC 232R: Week 3 Discussion**\n",
        "**Topic:** Apache Spark DataFrames\n",
        "\n",
        "This notebook covers the essentials of setting up a Spark environment in Colab, creating DataFrames, and performing basic to advanced transformations.\n",
        "\n",
        "**Key Concepts:**\n",
        "* SparkSession Entry Point\n",
        "* Schema Definition\n",
        "* Transformations vs. Actions\n",
        "* Immutability\n",
        "* UDFs (User Defined Functions)"
      ],
      "metadata": {
        "id": "intro_header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment Setup & Installation\n",
        "Since Google Colab does not have Spark installed by default, we need to install the Java Development Kit (JDK) and download Spark manually."
      ],
      "metadata": {
        "id": "jx80PfnxeYv7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N78xCpBVaOgL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0efd6169-0640-4cf3-ecbf-c608c8abf284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [1 InRelease 0 B/129 kB \u001b[0m\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:3 https://cli.github.com/packages stable InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.8 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,637 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,289 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,630 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,601 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,877 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,969 kB]\n",
            "Fetched 23.5 MB in 5s (5,039 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "14 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.12/dist-packages (0.10.9.9)\n"
          ]
        }
      ],
      "source": [
        "# Update apt-get repositories\n",
        "!sudo apt update\n",
        "\n",
        "# Install Java (OpenJDK 8) - Spark runs on the JVM\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download Apache Spark 3.2.1 with Hadoop 3.2\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "\n",
        "# Unzip the downloaded file\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "\n",
        "# Install findspark (helps locate Spark in the system)\n",
        "!pip install -q findspark\n",
        "\n",
        "# Install PySpark library\n",
        "!pip install pyspark\n",
        "\n",
        "# Install Py4J (enables Python to dynamically access Java objects)\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Initialize SparkSession\n",
        "The `SparkSession` is the entry point to programming Spark with the Dataset and DataFrame API."
      ],
      "metadata": {
        "id": "spark_session_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "oW_xuMwL2thp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a SparkSession\n",
        "# .master(\"local[*]\") sets the master to local mode using all available cores (*)\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DataFrame Tutorial\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "MZXILRJM4uhQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. DataFrames\n",
        "Apache Spark DataFrames are distributed collections of data organized into named columns.\n",
        "\n",
        "\n",
        "\n",
        "They are conceptually equivalent to tables in a relational database or DataFrames in Python's pandas library, but with the added benefit of being **distributed** across a cluster for big data processing.\n",
        "\n",
        "### Creating DataFrames"
      ],
      "metadata": {
        "id": "B2oVRSqg9AdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Method 1: Inferring Schema from a List\n",
        "Spark can automatically infer the data types based on the data provided."
      ],
      "metadata": {
        "id": "o4reXhuN6A1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame from a list of tuples\n",
        "data = [(\"John\", 25, \"New York\"),\n",
        "        (\"Jane\", 30, \"San Francisco\"),\n",
        "        (\"Mike\", 35, \"Chicago\")]\n",
        "\n",
        "# Define column names\n",
        "columns = [\"name\", \"age\", \"city\"]\n",
        "\n",
        "# Create the DataFrame (Spark infers types here)\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show the data (Action)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "-7NUswfZ4xD3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0643e45b-e727-4c37-fd05-1d9a3cc35a2d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-------------+\n",
            "|name|age|         city|\n",
            "+----+---+-------------+\n",
            "|John| 25|     New York|\n",
            "|Jane| 30|San Francisco|\n",
            "|Mike| 35|      Chicago|\n",
            "+----+---+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Method 2: Defining a Strict Schema\n",
        "For production pipelines, it is best practice to define a schema explicitly to ensure data integrity and improve performance."
      ],
      "metadata": {
        "id": "LgxP3cr36F1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "# Define schema using StructType and StructField\n",
        "# nullable=False means the field cannot contain Null values\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), nullable=False),\n",
        "    StructField(\"age\", IntegerType(), nullable=False),\n",
        "    StructField(\"city\", StringType(), nullable=True)\n",
        "])\n",
        "\n",
        "# Create DataFrame enforcing the defined schema\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "B9eSuYwe4z7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c46d906-488b-460a-af39-4114dca83e1d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-------------+\n",
            "|name|age|         city|\n",
            "+----+---+-------------+\n",
            "|John| 25|     New York|\n",
            "|Jane| 30|San Francisco|\n",
            "|Mike| 35|      Chicago|\n",
            "+----+---+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Method 3: Reading from External Sources\n",
        "Spark supports reading from various formats like CSV, JSON, Parquet, and JDBC.\n",
        "\n",
        "```python\n",
        "# CSV (Setting inferSchema=True forces Spark to read the file once to guess types)\n",
        "df_csv = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# JSON\n",
        "df_json = spark.read.json(\"path/to/file.json\")\n",
        "\n",
        "# Parquet (Efficient, columnar storage format - highly recommended for Spark)\n",
        "df_parquet = spark.read.parquet(\"path/to/file.parquet\")\n",
        "\n",
        "# Database (JDBC)\n",
        "df_jdbc = spark.read \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", \"jdbc:postgresql://localhost:5432/database\") \\\n",
        "    .option(\"dbtable\", \"table_name\") \\\n",
        "    .option(\"user\", \"username\") \\\n",
        "    .option(\"password\", \"password\") \\\n",
        "    .load()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "DWrgiNhs6JZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Basic DataFrame Operations\n",
        "Explaining **Actions** vs **Transformations**:\n",
        "* **Transformations** (e.g., `select`, `filter`) are *lazy*. They build a logical plan but don't execute immediately.\n",
        "* **Actions** (e.g., `show`, `count`) trigger the actual computation."
      ],
      "metadata": {
        "id": "QsYU7bpM6RD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Viewing Data"
      ],
      "metadata": {
        "id": "viewing_data_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first 20 rows by default (Action)\n",
        "df.show()\n",
        "\n",
        "# Display the first n rows\n",
        "df.show(5)\n",
        "\n",
        "# Show the schema (tree structure of columns and types)\n",
        "df.printSchema()\n",
        "\n",
        "# Get basic statistics (count, mean, stddev, min, max) - similar to pandas df.describe()\n",
        "df.describe().show()"
      ],
      "metadata": {
        "id": "GjSCn93549L7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5d68ee1-26cc-4c66-f425-f31156a06fa0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-------------+\n",
            "|name|age|         city|\n",
            "+----+---+-------------+\n",
            "|John| 25|     New York|\n",
            "|Jane| 30|San Francisco|\n",
            "|Mike| 35|      Chicago|\n",
            "+----+---+-------------+\n",
            "\n",
            "+----+---+-------------+\n",
            "|name|age|         city|\n",
            "+----+---+-------------+\n",
            "|John| 25|     New York|\n",
            "|Jane| 30|San Francisco|\n",
            "|Mike| 35|      Chicago|\n",
            "+----+---+-------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = false)\n",
            " |-- age: integer (nullable = false)\n",
            " |-- city: string (nullable = true)\n",
            "\n",
            "+-------+----+----+-------------+\n",
            "|summary|name| age|         city|\n",
            "+-------+----+----+-------------+\n",
            "|  count|   3|   3|            3|\n",
            "|   mean|NULL|30.0|         NULL|\n",
            "| stddev|NULL| 5.0|         NULL|\n",
            "|    min|Jane|  25|      Chicago|\n",
            "|    max|Mike|  35|San Francisco|\n",
            "+-------+----+----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selecting Columns\n",
        "You can select columns using string names or the `col()` function (useful for expressions)."
      ],
      "metadata": {
        "id": "0cGrI0KE6UkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Select a single column by name\n",
        "df.select(\"name\").show()\n",
        "\n",
        "# Select multiple columns\n",
        "df.select(\"name\", \"age\").show()\n",
        "\n",
        "# Select with expressions using F.col()\n",
        "# This allows arithmetic or other operations during selection\n",
        "df.select(F.col(\"name\"), F.col(\"age\") + 1).show()"
      ],
      "metadata": {
        "id": "13NBdBzq5F7E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27099fd5-b84b-4f5b-e044-1230fd7ea6fd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "|John|\n",
            "|Jane|\n",
            "|Mike|\n",
            "+----+\n",
            "\n",
            "+----+---+\n",
            "|name|age|\n",
            "+----+---+\n",
            "|John| 25|\n",
            "|Jane| 30|\n",
            "|Mike| 35|\n",
            "+----+---+\n",
            "\n",
            "+----+---------+\n",
            "|name|(age + 1)|\n",
            "+----+---------+\n",
            "|John|       26|\n",
            "|Jane|       31|\n",
            "|Mike|       36|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtering Data\n",
        "Filtering rows based on conditions (equivalent to SQL `WHERE`)."
      ],
      "metadata": {
        "id": "tg2_w1zr6XQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter by condition using column object\n",
        "df.filter(df.age > 25).show()\n",
        "\n",
        "# Multiple conditions using bitwise operators (& for AND, | for OR)\n",
        "df.filter((df.age > 25) & (df.city == \"Chicago\")).show()\n",
        "\n",
        "# Using SQL expression string directly\n",
        "df.filter(\"age > 25 AND city = 'Chicago'\").show()"
      ],
      "metadata": {
        "id": "_IHFCnk95H7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17c08054-5d73-403c-95eb-ec08b834384a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-------------+\n",
            "|name|age|         city|\n",
            "+----+---+-------------+\n",
            "|Jane| 30|San Francisco|\n",
            "|Mike| 35|      Chicago|\n",
            "+----+---+-------------+\n",
            "\n",
            "+----+---+-------+\n",
            "|name|age|   city|\n",
            "+----+---+-------+\n",
            "|Mike| 35|Chicago|\n",
            "+----+---+-------+\n",
            "\n",
            "+----+---+-------+\n",
            "|name|age|   city|\n",
            "+----+---+-------+\n",
            "|Mike| 35|Chicago|\n",
            "+----+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding and Modifying Columns\n",
        "**Note:** DataFrames are **immutable**. `withColumn` returns a *new* DataFrame with the requested change; it does not modify the original `df` in place."
      ],
      "metadata": {
        "id": "lzzJq2xG6Z-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new column based on an existing one\n",
        "df = df.withColumn(\"age_plus_ten\", df.age + 10)\n",
        "df.show()\n",
        "\n",
        "# Rename an existing column\n",
        "df = df.withColumnRenamed(\"age\", \"years_old\")\n",
        "df.show()\n",
        "\n",
        "# Drop a column\n",
        "df = df.drop(\"age_plus_ten\")\n",
        "df.show()"
      ],
      "metadata": {
        "id": "VakOpQuw5Jqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "870f55b1-e6bf-4346-b0eb-6515b8175e2e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-------------+------------+\n",
            "|name|age|         city|age_plus_ten|\n",
            "+----+---+-------------+------------+\n",
            "|John| 25|     New York|          35|\n",
            "|Jane| 30|San Francisco|          40|\n",
            "|Mike| 35|      Chicago|          45|\n",
            "+----+---+-------------+------------+\n",
            "\n",
            "+----+---------+-------------+------------+\n",
            "|name|years_old|         city|age_plus_ten|\n",
            "+----+---------+-------------+------------+\n",
            "|John|       25|     New York|          35|\n",
            "|Jane|       30|San Francisco|          40|\n",
            "|Mike|       35|      Chicago|          45|\n",
            "+----+---------+-------------+------------+\n",
            "\n",
            "+----+---------+-------------+\n",
            "|name|years_old|         city|\n",
            "+----+---------+-------------+\n",
            "|John|       25|     New York|\n",
            "|Jane|       30|San Francisco|\n",
            "|Mike|       35|      Chicago|\n",
            "+----+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Advanced DataFrame Operations"
      ],
      "metadata": {
        "id": "TA7O_X49_Z3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grouping and Aggregation\n",
        "Spark provides the `groupBy` transformation which returns a `GroupedData` object, on which you can perform aggregations."
      ],
      "metadata": {
        "id": "XHQWSMPY6csu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by 'city' and count the number of rows per city\n",
        "df.groupBy(\"city\").count().show()"
      ],
      "metadata": {
        "id": "i9yppugH5MZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2f5e78f-563e-4571-af68-2aa5ae5d1f8b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+\n",
            "|         city|count|\n",
            "+-------------+-----+\n",
            "|     New York|    1|\n",
            "|San Francisco|    1|\n",
            "|      Chicago|    1|\n",
            "+-------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform multiple aggregations at once using the .agg() method\n",
        "df.groupBy(\"city\").agg(\n",
        "    F.count(\"*\").alias(\"count\"),          # Count rows\n",
        "    F.avg(\"years_old\").alias(\"avg_age\"),  # Calculate average age\n",
        "    F.min(\"years_old\").alias(\"min_age\"),  # Find minimum age\n",
        "    F.max(\"years_old\").alias(\"max_age\")   # Find maximum age\n",
        ").show()"
      ],
      "metadata": {
        "id": "bfTgKk8wYBgg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afedb1dd-0ca1-43dc-c846-a89d3c40a553"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+-------+-------+-------+\n",
            "|         city|count|avg_age|min_age|max_age|\n",
            "+-------------+-----+-------+-------+-------+\n",
            "|     New York|    1|   25.0|     25|     25|\n",
            "|San Francisco|    1|   30.0|     30|     30|\n",
            "|      Chicago|    1|   35.0|     35|     35|\n",
            "+-------------+-----+-------+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Joining DataFrames\n",
        "Combining two DataFrames based on a common column."
      ],
      "metadata": {
        "id": "PjpBv5vJ6f0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 'employee' DataFrame\n",
        "employee_data = [\n",
        "    (1, \"John\", \"Engineering\"),\n",
        "    (2, \"Jane\", \"Marketing\"),\n",
        "    (3, \"Mike\", \"Sales\")\n",
        "]\n",
        "employee_df = spark.createDataFrame(employee_data, [\"id\", \"name\", \"department\"])\n",
        "\n",
        "# Create 'salary' DataFrame\n",
        "salary_data = [\n",
        "    (1, 70000),\n",
        "    (2, 80000),\n",
        "    (3, 65000)\n",
        "]\n",
        "salary_df = spark.createDataFrame(salary_data, [\"id\", \"salary\"])\n",
        "\n",
        "# Inner Join: Returns records that have matching values in both tables\n",
        "employee_df.join(salary_df, \"id\").show()\n",
        "\n",
        "# Left Join: Returns all records from the left table, and the matched records from the right table\n",
        "employee_df.join(salary_df, \"id\", \"left\").show()\n",
        "\n",
        "# Right Join: Returns all records from the right table, and the matched records from the left table\n",
        "employee_df.join(salary_df, \"id\", \"right\").show()\n",
        "\n",
        "# Full Outer Join: Returns all records when there is a match in either left or right table\n",
        "employee_df.join(salary_df, \"id\", \"outer\").show()"
      ],
      "metadata": {
        "id": "ZJrcdH805OQ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e541b40-642f-4cec-ec71-8aac3bbc7e79"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----------+------+\n",
            "| id|name| department|salary|\n",
            "+---+----+-----------+------+\n",
            "|  1|John|Engineering| 70000|\n",
            "|  2|Jane|  Marketing| 80000|\n",
            "|  3|Mike|      Sales| 65000|\n",
            "+---+----+-----------+------+\n",
            "\n",
            "+---+----+-----------+------+\n",
            "| id|name| department|salary|\n",
            "+---+----+-----------+------+\n",
            "|  1|John|Engineering| 70000|\n",
            "|  3|Mike|      Sales| 65000|\n",
            "|  2|Jane|  Marketing| 80000|\n",
            "+---+----+-----------+------+\n",
            "\n",
            "+---+----+-----------+------+\n",
            "| id|name| department|salary|\n",
            "+---+----+-----------+------+\n",
            "|  1|John|Engineering| 70000|\n",
            "|  3|Mike|      Sales| 65000|\n",
            "|  2|Jane|  Marketing| 80000|\n",
            "+---+----+-----------+------+\n",
            "\n",
            "+---+----+-----------+------+\n",
            "| id|name| department|salary|\n",
            "+---+----+-----------+------+\n",
            "|  1|John|Engineering| 70000|\n",
            "|  2|Jane|  Marketing| 80000|\n",
            "|  3|Mike|      Sales| 65000|\n",
            "+---+----+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Missing Values\n",
        "Spark provides the `DataFrameNaFunctions` submodule (accessed via `df.na`) to handle `null` or `NaN` values."
      ],
      "metadata": {
        "id": "ndcc_Sge6lbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame with explicit None (null) values\n",
        "data_with_nulls = [\n",
        "    (\"John\", 25, \"New York\"),\n",
        "    (\"Jane\", None, \"San Francisco\"),\n",
        "    (\"Mike\", 35, None),\n",
        "    (None, 40, \"Boston\")\n",
        "]\n",
        "df_nulls = spark.createDataFrame(data_with_nulls, [\"name\", \"age\", \"city\"])\n",
        "\n",
        "# drop(): Removes rows that contain null values\n",
        "df_nulls.na.drop().show()\n",
        "\n",
        "# drop(subset=...): Removes rows where specific columns are null\n",
        "df_nulls.na.drop(subset=[\"name\"]).show()\n",
        "\n",
        "# fill(): Replaces null values with specified values\n",
        "df_nulls.na.fill({\"age\": 0, \"name\": \"Unknown\", \"city\": \"Unknown\"}).show()"
      ],
      "metadata": {
        "id": "8capSclM5Qie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd5f0f12-7cbb-4108-8274-fdff148709e8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+--------+\n",
            "|name|age|    city|\n",
            "+----+---+--------+\n",
            "|John| 25|New York|\n",
            "+----+---+--------+\n",
            "\n",
            "+----+----+-------------+\n",
            "|name| age|         city|\n",
            "+----+----+-------------+\n",
            "|John|  25|     New York|\n",
            "|Jane|NULL|San Francisco|\n",
            "|Mike|  35|         NULL|\n",
            "+----+----+-------------+\n",
            "\n",
            "+-------+---+-------------+\n",
            "|   name|age|         city|\n",
            "+-------+---+-------------+\n",
            "|   John| 25|     New York|\n",
            "|   Jane|  0|San Francisco|\n",
            "|   Mike| 35|      Unknown|\n",
            "|Unknown| 40|       Boston|\n",
            "+-------+---+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### User Defined Functions (UDFs)\n",
        "UDFs allow you to extend Spark's functionality by registering standard Python functions.\n",
        "\n",
        "*Note: UDFs can be slower than native Spark functions because they require serializing data between the JVM and Python. Use native functions (like `F.when`, `F.col`) whenever possible.*"
      ],
      "metadata": {
        "id": "ZBN406EmGqi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "df.show()\n",
        "\n",
        "# 1. Define a standard Python function\n",
        "def age_category(age):\n",
        "    if age is None:\n",
        "        return \"Unknown\"\n",
        "    elif age < 30:\n",
        "        return \"Young\"\n",
        "    else:\n",
        "        return \"Senior\"\n",
        "\n",
        "# 2. Register the function as a Spark UDF, specifying the return type\n",
        "age_category_udf = udf(age_category, StringType())\n",
        "\n",
        "# 3. Apply the UDF to a column\n",
        "df.withColumn(\"age_category\", age_category_udf(df.years_old)).show()"
      ],
      "metadata": {
        "id": "bc4dfhZXGqFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88679c59-60c5-4b4e-a400-d974d0c21eb6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+-------------+\n",
            "|name|years_old|         city|\n",
            "+----+---------+-------------+\n",
            "|John|       25|     New York|\n",
            "|Jane|       30|San Francisco|\n",
            "|Mike|       35|      Chicago|\n",
            "+----+---------+-------------+\n",
            "\n",
            "+----+---------+-------------+------------+\n",
            "|name|years_old|         city|age_category|\n",
            "+----+---------+-------------+------------+\n",
            "|John|       25|     New York|       Young|\n",
            "|Jane|       30|San Francisco|      Senior|\n",
            "|Mike|       35|      Chicago|      Senior|\n",
            "+----+---------+-------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Exporting Data\n",
        "Writing DataFrames to storage.\n",
        "\n",
        "```python\n",
        "# Save as CSV\n",
        "df.write.csv(\"path/to/output/csv\", header=True)\n",
        "\n",
        "# Save as JSON\n",
        "df.write.json(\"path/to/output/json\")\n",
        "\n",
        "# Save as Parquet (Preserves schema and is compressed)\n",
        "df.write.parquet(\"path/to/output/parquet\")\n",
        "\n",
        "# Save to a database via JDBC\n",
        "df.write \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", \"jdbc:postgresql://localhost:5432/database\") \\\n",
        "    .option(\"dbtable\", \"output_table\") \\\n",
        "    .option(\"user\", \"username\") \\\n",
        "    .option(\"password\", \"password\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "SpOqvFWpGafG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resources For DataFrames\n",
        "1.  [Spark By Examples - PySpark Tutorial](https://sparkbyexamples.com/pyspark-tutorial/)\n",
        "2.  [Official PySpark Documentation](https://spark.apache.org/docs/latest/api/python/index.html)"
      ],
      "metadata": {
        "id": "BFoFod6iDvS4"
      }
    }
  ]
}