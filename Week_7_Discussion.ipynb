{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhovN2p_i8TA"
      },
      "source": [
        "# DSC 232R: Week 7 Discussion\n",
        "## Ensemble Methods: Boosting, Margins, and Applications\n",
        "Today we will cover the core theoretical concepts behind Boosting, including how it compares to other models, why it resists overfitting, and its real-world applications. Afterwards, we will jump into PySpark code to implement these concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RhAgfk8i8TB"
      },
      "source": [
        "### 1. Generative vs. Discriminative Models\n",
        "* **Generative Models:** goal is to explain how data is generated. Generative models are more accurate when they are correct.\n",
        "* **Discriminative Models:** goal is to predict a property of the data (such as label). Discriminative models are more robust against poor modeling or outliers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTlBeuadi8TB"
      },
      "source": [
        "### 2. Introduction to Boosting and Weak Learners\n",
        "* **Boosting Concept:** Adaboost is an algorithm for combining weak learners. The resulting weighted majority rule is more accurate than any of the weak rules.\n",
        "* **Focus on Hard Examples:** Intuitively: boosting concentrates on the hard examples.\n",
        "* **Weak Learners:** Almost anything can be a weak learner.\n",
        "\n",
        "Popular choices: Boosting Trees, Boosting Stumps, Alternating Decision Trees\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc10Jbi2i8TC"
      },
      "source": [
        "### 3. AdaBoost and Gradient Descent\n",
        "\n",
        "* **AdaBoost vs. LogitBoost:**\n",
        "  * Adaboost: performs well when a achievable error rate is close to zero (almost consistent case).  Errors = examples with negative margins, get very large weights, can overfit.\n",
        "  * Logitboost: Inferior to adaboost when achievable error rate is close to zero.  Often better than Adaboost when achievable error rate is high.  Weight on any example never larger than 1.\n",
        "\n",
        "\n",
        "* **Loss Functions:** AdaBoost minimizes the exponential loss, which acts as a strict upper bound on the 0-1 (classification) loss.\n",
        "  $$L_{AdaBoost} = e^{-y \\cdot f(x)}$$\n",
        "* **LogitBoost Loss:** Uses a logistic loss function, which grows linearly rather than exponentially for large negative margins, making it more robust to outliers.\n",
        "  $$L_{LogitBoost} = \\log(1 + e^{-y \\cdot f(x)})$$\n",
        "* **Gradient Descent:** AdaBoost finds the weight vector $\\alpha$ through coordinate-wise gradient descent, adding weak rules one at a time to minimize this loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp1z2okEi8TC"
      },
      "source": [
        "### 4. Over-fitting, Margins, and The Bias/Variance Tradeoff\n",
        "\n",
        "\n",
        "* **Sources of Error:**\n",
        "  * *Model Bias:* error resulting from the inability of the model class to represent the true distribution.\n",
        "  * *Data Variation:* error resulting from the difference between the training set and the true distribution.\n",
        "* **Bagging vs. Boosting:**\n",
        "  * Bagging decreases data variation without significantly increasing model bias.\n",
        "  * Boosting reduces both variation and model bias.\n",
        "\n",
        "\n",
        "* **The Definition of Margin:** In binary classification where $y \\in \\{-1, +1\\}$, the margin is defined as $y \\cdot f(x)$. A positive margin means a correct prediction; a negative margin means a mistake.\n",
        "* **Resistance to Over-fitting:** Boosting pushes to maximize this margin. Even after achieving 0 training error, boosting continues to run to push the margins wider, meaning small changes in the test set won't cause the prediction to flip."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1VVQtgci8TD"
      },
      "source": [
        "### 5. Applications of Boosting\n",
        "* **Voice Request Classification:** Classify voice requests Voice -> text -> category\n",
        "* **Face Detection**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBoE-XNDi8TD"
      },
      "source": [
        "### 6. PySpark Implementation: Boosting vs. Bagging\n",
        "In this section, we implement Bagging (Random Forest) and Boosting (Gradient-Boosted Trees) using PySpark MLlib, allowing us to observe these concepts in a distributed Big Data context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWhTXrtri8TE",
        "outputId": "1cb00a2e-1915-407c-c95a-4ab09b20699c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training rows: 387, Testing rows: 113\n",
            "+-----+--------------------+------------------+--------------------+\n",
            "|label|            feature1|          feature2|            features|\n",
            "+-----+--------------------+------------------+--------------------+\n",
            "|    0|0.026757226028857328| 4.505037046177024|[0.02675722602885...|\n",
            "|    0| 0.09669699608339966|0.7524386007376704|[0.09669699608339...|\n",
            "|    0|  0.2366443470145152|1.9312978832770866|[0.23664434701451...|\n",
            "|    0| 0.24786361898188725| 7.365644717550821|[0.24786361898188...|\n",
            "|    0|  0.2567842549746602| 3.119572443946952|[0.25678425497466...|\n",
            "+-----+--------------------+------------------+--------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ],
      "source": [
        "# Setup PySpark session for the discussion\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "import random\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('DSC232R_Week7_Boosting_vs_Bagging') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Generate a larger synthetic dataset with ~15% inherent noise\n",
        "random.seed(42)\n",
        "data = []\n",
        "for _ in range(500):\n",
        "    f1 = random.uniform(0, 10)\n",
        "    f2 = random.uniform(0, 10)\n",
        "\n",
        "    # Base decision boundary: if f1 + f2 > 10, label is 1, else 0\n",
        "    label = 1 if (f1 + f2) > 10 else 0\n",
        "\n",
        "    # Introduce noise: flip the true label 15% of the time to simulate data variation\n",
        "    if random.random() < 0.15:\n",
        "        label = 1 - label\n",
        "\n",
        "    data.append((label, f1, f2))\n",
        "\n",
        "columns = ['label', 'feature1', 'feature2']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Assemble features into a vector\n",
        "assembler = VectorAssembler(inputCols=['feature1', 'feature2'], outputCol='features')\n",
        "df_assembled = assembler.transform(df)\n",
        "\n",
        "# Split data into training and test sets\n",
        "train_df, test_df = df_assembled.randomSplit([0.75, 0.25], seed=42)\n",
        "print(f\"Training rows: {train_df.count()}, Testing rows: {test_df.count()}\")\n",
        "train_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyUmlZm_i8TE",
        "outputId": "46f78638-c158-426f-90a6-839aa30909c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest (Bagging) Accuracy: 0.78\n",
            "Gradient-Boosted Trees (Boosting) Accuracy: 0.75\n"
          ]
        }
      ],
      "source": [
        "# 1. Bagging: Random Forest Classifier\n",
        "# Bagging decreases data variation (variance) without significantly increasing model bias.\n",
        "# It handles the 15% noise well by taking a majority vote across multiple randomized trees.\n",
        "rf = RandomForestClassifier(featuresCol='features', labelCol='label', numTrees=20, maxDepth=5, seed=42)\n",
        "rf_model = rf.fit(train_df)\n",
        "rf_predictions = rf_model.transform(test_df)\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
        "rf_accuracy = evaluator.evaluate(rf_predictions)\n",
        "print(f'Random Forest (Bagging) Accuracy: {rf_accuracy:.2f}')\n",
        "\n",
        "# 2. Boosting: Gradient-Boosted Trees (GBT)\n",
        "# Boosting reduces both variation and model bias by iteratively focusing on hard examples.\n",
        "# However, if the data is highly noisy, it can sometimes overfit by trying too hard to correct the unavoidable noise.\n",
        "gbt = GBTClassifier(featuresCol='features', labelCol='label', maxIter=20, maxDepth=3, seed=42)\n",
        "gbt_model = gbt.fit(train_df)\n",
        "gbt_predictions = gbt_model.transform(test_df)\n",
        "\n",
        "gbt_accuracy = evaluator.evaluate(gbt_predictions)\n",
        "print(f'Gradient-Boosted Trees (Boosting) Accuracy: {gbt_accuracy:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discussion: Why did Random Forest beat Gradient-Boosted Trees?\n",
        "\n",
        "You might be surprised to see that Random Forest (Bagging) achieved a higher accuracy (0.78) compared to Gradient-Boosted Trees (0.75). Since boosting is often considered a more powerful technique that reduces both bias and variance, why did it lose here?\n",
        "\n",
        "The answer lies in the **15% random noise** we injected into the dataset.\n",
        "\n",
        "1. **Bagging's Strength:** Random Forest builds independent trees. By taking a majority vote, the random noise naturally averages out, making the model highly robust to messy data.\n",
        "2. **Boosting's Weakness:** Boosting algorithms (like GBT and AdaBoost) are designed to relentlessly focus on misclassified examples by increasing their weights. When a dataset has inherent noise, the boosting algorithm assumes these noisy points are just hard to learn examples. It dedicates all its resources to fitting this random noise (points with negative margins), leading to **overfitting**.\n",
        "\n",
        "**Key Takeaway:** While boosting pushes for larger margins and typically yields highly accurate models on clean data, it can be highly sensitive to noise. If your achievable error rate is high due to messy data, bagging or a modified boosting loss function (like LogitBoost) is often the safer choice!"
      ],
      "metadata": {
        "id": "y-NSki3PT9x6"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}